{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQKMqkmHD5tQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrj5On3d8YjF"
      },
      "source": [
        "Since the data augmentation and dropout are applied to the model, the result will be non-deterministic. Set the random seed to a number to get the same result every time you run the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqGBUtP28WEa"
      },
      "outputs": [],
      "source": [
        "seed = 10\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc6Klx4g2b0d"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OjhNl3d2iNj"
      },
      "source": [
        "The Flickr8K dataset will be used in this notebook. It comprises over 8,000 images, that are each paired with five different captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1bPkO9PEQNi"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip \n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip \n",
        "!unzip -qq Flickr8k_Dataset.zip -d data\n",
        "!unzip -qq Flickr8k_text.zip -d data\n",
        "!rm Flickr8k_Dataset.zip Flickr8k_text.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s75SUF5hElf4"
      },
      "outputs": [],
      "source": [
        "# Path to the images\n",
        "IMAGES_PATH = \"data/Flicker8k_Dataset\"\n",
        "\n",
        "# Desired image dimensions\n",
        "IMAGE_SIZE = (300, 300)\n",
        "\n",
        "# Number of captions\n",
        "NUM_CAPTIONS = 5\n",
        "\n",
        "# Vocabulary size\n",
        "VOCAB_SIZE = 8000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "SEQ_LENGTH = 40\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "EMBED_DIM = 100\n",
        "\n",
        "# LSTM units \n",
        "UNITS = 512\n",
        "\n",
        "# Other training parameters\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLGZqIjb28Io"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdp_XtW6Fcr7"
      },
      "outputs": [],
      "source": [
        "def load_captions_data(filename):\n",
        "    \"\"\"Load data from text file and map each image name to corresponding captions.\n",
        "\n",
        "    Args:\n",
        "        filename (string): Path to the text file containing image-caption data.\n",
        "\n",
        "    Returns:\n",
        "        captions_mapping (dict): Dictionary mapping image names and the corresponding captions\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename) as caption_file:\n",
        "        caption_data = caption_file.readlines()\n",
        "        captions_mapping = {}\n",
        "\n",
        "        for line in caption_data:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            # Image name and captions are separated using a tab\n",
        "            img_name, caption = line.split(\"\\t\")\n",
        "\n",
        "            # Each image is repeated five times for the five different captions.\n",
        "            # Each image name has a suffix `#(caption_number)`\n",
        "            img_name = img_name.split(\"#\")[0]\n",
        "            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
        "\n",
        "            if img_name.endswith(\"jpg\"):\n",
        "                # We will add a start and an end token to each caption\n",
        "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
        "               \n",
        "                if img_name in captions_mapping:\n",
        "                    captions_mapping[img_name].append(caption)\n",
        "                else:\n",
        "                    captions_mapping[img_name] = [caption]\n",
        "\n",
        "        return captions_mapping\n",
        "\n",
        "\n",
        "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
        "    \"\"\"Split the captioning dataset into train and validation sets.\n",
        "\n",
        "    Args:\n",
        "        caption_data (dict): Dictionary containing the mapped caption data\n",
        "        train_size (float): Fraction of all the full dataset to use as training data\n",
        "        shuffle (bool): Whether to shuffle the dataset before splitting\n",
        "\n",
        "    Returns:\n",
        "        Traning and validation datasets as two separated dicts\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Get the list of all image names\n",
        "    all_images = list(caption_data.keys())\n",
        "\n",
        "    # 2. Shuffle if necessary\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    # 3. Split into training and validation sets\n",
        "    train_size = int(len(caption_data) * train_size)\n",
        "\n",
        "    training_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
        "    }\n",
        "    validation_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
        "    }\n",
        "\n",
        "    # 4. Return the splits\n",
        "    return training_data, validation_data\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "captions_mapping = load_captions_data(\"data/Flickr8k.token.txt\")\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, valid_data = train_val_split(captions_mapping)\n",
        "print(\"Number of training samples: \", len(train_data))\n",
        "print(\"Number of validation samples: \", len(valid_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPl8bRpi3GgJ"
      },
      "source": [
        "## Vectorizing the text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrthtybCGAxy"
      },
      "outputs": [],
      "source": [
        "# Punctuations which are not allowed except < and >\n",
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"<\", \"\")\n",
        "strip_chars = strip_chars.replace(\">\", \"\")\n",
        "\n",
        "# Turn characters to lower case and remove the punctuations from the text\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "# Turn each word to a interger index\n",
        "vectorization = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "# Build a vocab from the training data\n",
        "text_data = list(train_data.values())\n",
        "text_data = np.array(text_data)\n",
        "text_data = np.reshape(text_data, (-1,))\n",
        "vectorization.adapt(text_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P5fYp7svjM3"
      },
      "outputs": [],
      "source": [
        "len(vectorization.get_vocabulary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-iGEGOydOTR"
      },
      "source": [
        "# Build tf.data.Dataset pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hqxara1JGFLI"
      },
      "outputs": [],
      "source": [
        "def decode_and_resize(img_path):\n",
        "    \"\"\"Read an image from image path, resize it and turn it to an array\n",
        "\n",
        "    Args:\n",
        "        img_path (string): Path to the image file\n",
        "\n",
        "    Returns:\n",
        "        A 3D-array where each elements a pixel value of the image\n",
        "    \"\"\"   \n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def format_dataset(img_path, caption):\n",
        "    \"\"\"Format the dataset for training model\n",
        "\n",
        "    Args:\n",
        "        img_path (string): Path to the image file\n",
        "        caption (string): The corresponding caption of the image\n",
        "\n",
        "    Returns:\n",
        "        A pair of a 3D-array of the image and the vectorized caption\n",
        "\n",
        "    \"\"\"\n",
        "    return decode_and_resize(img_path), vectorization(caption)\n",
        "\n",
        "\n",
        "def make_dataset(img_paths, captions):\n",
        "    \"\"\" Build batches of data from a set of image paths and a set of captions\n",
        "\n",
        "    Args:\n",
        "        img_paths (list): Paths to the image file\n",
        "        captions (list): The corresponding captions of the image\n",
        "\n",
        "    Returns:\n",
        "        An iterator where contains batches of data\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_paths, captions))\n",
        "    dataset = dataset.shuffle(len(img_paths))\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Pass the list of images and the list of corresponding captions\n",
        "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ8-kC3BmQSO"
      },
      "outputs": [],
      "source": [
        "# train_dataset is an iterator containing multiple batches\n",
        "# Each batch is an tuple of images and captions.  \n",
        "# Get 1 random batch and check the dimension of each element\n",
        "for (images, captions) in train_dataset.take(1):\n",
        "  print(images.shape)\n",
        "  print(captions.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK5KdUHEFSkQ"
      },
      "source": [
        "# Glove word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLLtQ4FlFMgu"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip -d glove\n",
        "!rm glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOUh2lbaDw6T"
      },
      "outputs": [],
      "source": [
        "path_to_glove_file = \"glove/glove.6B.100d.txt\"\n",
        "embeddings_index = {}\n",
        "\n",
        "with open(path_to_glove_file) as f:\n",
        "  for line in f:\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR7PBFhzD2Eq"
      },
      "outputs": [],
      "source": [
        "vocab = vectorization.get_vocabulary()\n",
        "word_index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, EMBED_DIM)) \n",
        "\n",
        "# Create a embedding matrix based on learned semantic features for all the words in the vocab\n",
        "for i, word, in word_index_lookup.items():\n",
        "  if i < VOCAB_SIZE:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None: \n",
        "    embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irpmaXwCsUCO"
      },
      "source": [
        "# Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1xwtAaLrt9f"
      },
      "source": [
        "**Load pre-trained model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcoIiLWHqr2X"
      },
      "outputs": [],
      "source": [
        "def get_cnn_model():\n",
        "    base_model = EfficientNetV2B0(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\")\n",
        "    # Freeze the feature extractor so that the weights are not updated during training\n",
        "    base_model.trainable = False\n",
        "    base_model_out = base_model.output   # Shape (10,10,1280)\n",
        "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)  # Shape (100, 1280)\n",
        "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
        "    return cnn_model\n",
        "\n",
        "\n",
        "# Data augmentation for image data\n",
        "image_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.2),\n",
        "        layers.RandomContrast(0.3)\n",
        "    ]\n",
        ")\n",
        "\n",
        "cnn_model = get_cnn_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbRqUYgJqnbp"
      },
      "source": [
        "To see the effect of data augmentation, the random seed defined at the beginning of the notebook have to be removed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFTa6OE623_D"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        augmented_images = image_augmentation(images)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbKzxsm8sMni"
      },
      "source": [
        "**Attention mechanism & Positional Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05S1Bj0WqtW_"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = layers.Dense(units)\n",
        "        self.W2 = layers.Dense(units)\n",
        "        self.V = layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        # Aligment-model\n",
        "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                             self.W2(hidden_with_time_axis)))\n",
        "        # Aligment score\n",
        "        score = self.V(attention_hidden_layer)\n",
        "\n",
        "        # Produce attended context vector\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        # Embedding layer for the token indices\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim,\n",
        "            output_dim=output_dim,\n",
        "            embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "            mask_zero=True)\n",
        "        # Embedding layer for the token positions\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        \n",
        "    def call(self, inputs, position):\n",
        "        embedded_tokens = self.token_embeddings(inputs)      \n",
        "        embedded_positions = self.position_embeddings(position)\n",
        "        # Add both embedding vectors together\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    # Generate a mask so padding 0s in the inputs can be ignored. \n",
        "    # The method will called automatically by the framework, and the \n",
        "    # mask will get propagated to the next layer\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtrwte-EscJl"
      },
      "source": [
        "**Encoder-Decoder Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RTqjJyvgGI3"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(keras.Model):\n",
        "    def __init__(self, cnn_model, embed_dim, image_augmentation, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.cnn_model = cnn_model\n",
        "        self.embed_dim = embed_dim\n",
        "        self.image_augmentation = image_augmentation\n",
        "        self.preprocess_input = keras.applications.efficientnet_v2.preprocess_input\n",
        "        self.dropout = layers.Dropout(0.5)\n",
        "        self.fc = layers.Dense(embed_dim, activation=\"relu\")\n",
        "    def call(self, inputs):\n",
        "        x = self.image_augmentation(inputs) \n",
        "        x = self.preprocess_input(x)    \n",
        "        x = cnn_model(x)                  \n",
        "        x = self.dropout(x)\n",
        "        img_feature = self.fc(x)         \n",
        "        return img_feature\n",
        "\n",
        "class CaptionDecoder(keras.Model):\n",
        "    def __init__(self, sequence_length, embedding_dim, units, vocab_size, embedding_matrix):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        self.positional_embeding = PositionalEmbedding(sequence_length, vocab_size, embedding_dim)  \n",
        "        self.lstm = layers.LSTM(self.units,\n",
        "                                return_sequences=True,\n",
        "                                return_state=True,\n",
        "                                recurrent_initializer='glorot_uniform')\n",
        "        self.dropout = layers.Dropout(0.25)\n",
        "        self.layer_normalize = layers.LayerNormalization()\n",
        "        self.fc = layers.Dense(vocab_size)\n",
        "        self.activator = layers.Activation(\"softmax\")\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, inputs, features, hidden_state, position):\n",
        "        # defining attention as a separate model\n",
        "        context_vector, attention_weights = self.attention(features, hidden_state)\n",
        "        # word turned into a vector with position information\n",
        "        x = self.positional_embeding(inputs, position)\n",
        "        # word vector and context vector a concatenated\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        # passing the concatenated vector to the LSTM\n",
        "        output, hidden_state, _ = self.lstm(x)        \n",
        "  \n",
        "        x = self.dropout(output)\n",
        "        x = self.fc(x)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "        x = self.layer_normalize(x)\n",
        "    \n",
        "        # ouput a probability distribution over words in vocab\n",
        "        output = self.activator(x)\n",
        "\n",
        "        return output, hidden_state, attention_weights\n",
        "    \n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))\n",
        "\n",
        "encoder = ImageEncoder(cnn_model, EMBED_DIM, image_augmentation)\n",
        "decoder = CaptionDecoder(SEQ_LENGTH, EMBED_DIM, UNITS, VOCAB_SIZE, embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vv--OEmrA7l"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "acc_fn = tf.keras.metrics.SparseCategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg5WKPNUqKZu"
      },
      "source": [
        "## Load the pretrained weights into the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O3y52CttTDa"
      },
      "source": [
        "The model has already been trained and has obtained a good result. You just need to load the trained weights into the model and run the evaluation to see the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdzOaRivqP7x"
      },
      "outputs": [],
      "source": [
        "!unzip -qq weights_efficientnetv2b0_finetuned.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I430FKbqQL_"
      },
      "outputs": [],
      "source": [
        "encoder.load_weights(\"weights/encoder_weights_efficientnetv2b0\")\n",
        "decoder.load_weights(\"weights/decoder_weights_efficientnetv2b0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4PMTDBngJ00"
      },
      "outputs": [],
      "source": [
        "#del cnn_model\n",
        "#del encoder\n",
        "#del decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ladXg1XlevD"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvRgTqaiq2bq"
      },
      "source": [
        "If the pre-trained weights are already loaded into the model, this training process and fine-tuning can be skipped.\n",
        "The entire training process can be summarized as follows.\n",
        "- 3 epochs with the frozen CNN-model (learning_rate = 1e-3)\n",
        "- 30 epochs with the unfrozen CNN-model and frozen Decoder (learning_rate = 1e-5)\n",
        "- 5 epochs with the unfrozen CNN-model and Decoder (learning_rate = 1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P86yzSAAgGas"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(image, target):\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    # initializing the hidden state for each batch\n",
        "    # so that the decoder doesn't continue with the data in the previous batch\n",
        "    hidden_state = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "    # a start token is considered as the first input for the decoder in a batch\n",
        "    dec_input = tf.expand_dims(vectorization([\"<start>\"] * target.shape[0])[:,0], 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Features are first extracted by the encoder \n",
        "        features = encoder(image)\n",
        "\n",
        "      # For position in the processing caption\n",
        "        for position in range(1, target.shape[1]):\n",
        "            # Passing the word, features, hidden state, position through the decoder\n",
        "            predictions, hidden_state, attention_weights = decoder(dec_input, features, hidden_state, position)\n",
        "\n",
        "            # Compute the loss and accuracy\n",
        "            loss += loss_fn(target[:, position], predictions)\n",
        "            acc_fn.update_state(target[:, position], predictions)\n",
        "            acc += acc_fn.result() \n",
        "            \n",
        "            # Using teacher forcing\n",
        "            # The model learns the correct word in each time step, not the predicted word is\n",
        "            dec_input = tf.expand_dims(target[:, position], 1)\n",
        "\n",
        "    total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "    total_acc = (acc / int(target.shape[1]))\n",
        "    \n",
        "    # Get the weights of the model\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # Calculate the gradients of the loss with respect to the weights\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    # Update the gradients by the optimizer\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return loss, total_loss , acc, total_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud-HJ0sbl6r_"
      },
      "outputs": [],
      "source": [
        "loss_plot = []\n",
        "acc_plot = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkUOXrm6l75_"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(0, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    num_steps = 0\n",
        "\n",
        "    for (batch, (image, target)) in enumerate(train_dataset):\n",
        "      # Train model with all 5 captions\n",
        "        for i in range(NUM_CAPTIONS):\n",
        "            batch_loss, t_loss, batch_acc, t_acc = train_step(image, target[:, i, :])\n",
        "            total_loss += t_loss\n",
        "            total_acc += t_acc\n",
        "\n",
        "        num_steps += 1\n",
        "        \n",
        "        # Print the average loss and acc every 100 batches\n",
        "        if batch % 100 == 0:\n",
        "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "            average_batch_acc = batch_acc.numpy()/int(target.shape[1])\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f} Acc {average_batch_acc:.4f}')\n",
        "\n",
        "    current_loss = total_loss / num_steps\n",
        "  \n",
        "    current_acc = total_acc / num_steps\n",
        "\n",
        "    # Storing the epoch end loss value to plot later\n",
        "    loss_plot.append(current_loss)\n",
        "\n",
        "    acc_plot.append(current_acc)\n",
        "  \n",
        "    print ('Epoch: {} - Loss: {:.6f} - Acc: {:.6f} - Time taken: {:.1f} sec'.format(\n",
        "        epoch + 1,\n",
        "        current_loss,\n",
        "        current_acc,\n",
        "        time.time() - start))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTEYxVusmKBP"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "plt.plot(range(1, len(loss_plot)+1) ,loss_plot, label=\"Training loss\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.legend()\n",
        "plt.savefig('loss_plot.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctHwodnW0lFh"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "plt.plot(range(1, len(loss_plot)+1), acc_plot, label=\"Training accuracy\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Acc')\n",
        "plt.title('Acc Plot')\n",
        "plt.legend()\n",
        "plt.savefig('acc_plot.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD7OtUKjkutA"
      },
      "source": [
        "## Fine-tuning the pretrained EfficientNetV2B0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMHwW7Fxt-5d"
      },
      "source": [
        "After training the model in the first 3 epochs, fine tune the EfficientNetV2B0 in 30 epochs. Run the code in the section \"Training loop\" (the third one) and set the EPOCHS variable to 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcYTxuc8kwkn"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning a portion of layers of the pretrained-CNN\n",
        "for layer in encoder.cnn_model.layers[-10:]:\n",
        "  if not isinstance(layer, layers.BatchNormalization):\n",
        "    layer.trainable = True\n",
        "\n",
        "# Decoder will not be updated \n",
        "decoder.trainable = False\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW2CtziMk_Oe"
      },
      "outputs": [],
      "source": [
        "# Check if the decoder is frozen\n",
        "assert len(decoder.trainable_variables) == 0, \"decoder should be frozen!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipK49fRhugjW"
      },
      "source": [
        "The decoder will be unfrozen and the model is trained further in 5 epochs. Run the code in the section \"Training loop\" (the third one) and set the EPOCHS variable to 5.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lttm5zyYk1sj"
      },
      "outputs": [],
      "source": [
        "decoder.trainable = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK4WbhnRz5kC"
      },
      "source": [
        "# Save model & weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLaVDfKbz4lh"
      },
      "outputs": [],
      "source": [
        "encoder.save_weights(\"weights/encoder_weights_efficientnetv2b0\")\n",
        "decoder.save_weights(\"weights/decoder_weights_efficientnetv2b0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNx5Qrrcz-ki"
      },
      "outputs": [],
      "source": [
        "!zip -r weights.zip weights/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAjdyMv60QDd"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFkLx0RzwkI8"
      },
      "outputs": [],
      "source": [
        "def generate_caption(img):\n",
        "    # Extract feature from an image\n",
        "    feature = encoder(tf.expand_dims(img, axis=0))\n",
        "\n",
        "    # Prepare an array to store attention weights of each predicted word\n",
        "    attention_features_shape = feature.shape[1]\n",
        "    attention_plot = np.zeros((SEQ_LENGTH, attention_features_shape))\n",
        "\n",
        "    # Init the hidden state of the decoder\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    # Prepare the start token as input for the decoder\n",
        "    dec_input = tf.expand_dims([vectorization([\"<start>\"])[0][0]], 1)\n",
        "    decoded_caption = \"<start>\"\n",
        "\n",
        "    for position in range(SEQ_LENGTH):\n",
        "        # Passing the word, features, hidden state, position through the decoder\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\n",
        "                                                         feature,\n",
        "                                                         hidden, position)\n",
        "        # Store the attention weights for later plot\n",
        "        attention_plot[position] = tf.reshape(attention_weights, (-1,)).numpy()\n",
        "\n",
        "        # Choose the word with the highst probability as predicted word\n",
        "        predicted_id = np.argmax(predictions)\n",
        "        # Turn the predicted word index to word as string\n",
        "        predicted_word = word_index_lookup[predicted_id]\n",
        "\n",
        "        # Concat each predicted word to form a caption\n",
        "        decoded_caption += \" \" + predicted_word\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "        \n",
        "        # The model finishes the caption\n",
        "        if predicted_word == \"<end>\":\n",
        "          break\n",
        "\n",
        "    # Clean the generated caption by remove start and end token    \n",
        "    decoded_caption = decoded_caption.replace(\"<start>\", \"\")\n",
        "    decoded_caption = decoded_caption.replace(\"<end>\", \"\").strip()\n",
        "\n",
        "    # The length of the generated caption does not always reach the maximum (= 40)\n",
        "    # Remove zeros from the attention's storage\n",
        "    attention_plot = attention_plot[:len(decoded_caption), :]\n",
        "    return decoded_caption, attention_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sucXCQvIv598"
      },
      "outputs": [],
      "source": [
        "def prepare_references(captions):\n",
        "  \"\"\"Build a list of references from the  captions.\n",
        "    Args:\n",
        "        captions: a list of a list of 5 captions -> [capA1,..., capA5]\n",
        "    Returns:\n",
        "        references: a list of references where each reference is a list of token \n",
        "        -> [refA1,...,refA5] with refAi = ['token1', 'token2',..]\n",
        "    \"\"\"\n",
        "  references = []\n",
        "  for caption in captions:\n",
        "    # Clean the caption\n",
        "    caption = caption.replace(\"<start>\",\"\")\n",
        "    caption = caption.replace(\"<end>\",\"\")\n",
        "    caption = caption.replace(\",\",\"\")\n",
        "    caption = caption.replace(\".\",\"\").strip()\n",
        "    # Split the caption into list of tokens\n",
        "    caption = caption.split()\n",
        "    references.append(caption)\n",
        "  return references"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUnc6TltnNvi"
      },
      "source": [
        "Running this cell below could take about 10 minutes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POzpqZ09mKWh"
      },
      "outputs": [],
      "source": [
        "# Get all the validation image path\n",
        "valid_images = list(valid_data.keys())\n",
        "\n",
        "def evaluate_model(valid_images, captions_mapping):\n",
        "    \"\"\"Calculate BLEU-1 to BLEU-4 Score for a list of captions as a text corpus.\n",
        "\n",
        "    Args:\n",
        "        valid_images (list): list of image paths\n",
        "        captions_mapping (dict): dictionary maps an image path to a list of captions\n",
        "    \"\"\"\n",
        "    references_list = []\n",
        "    predictions = []\n",
        "    for image_path in valid_images:\n",
        "        # Prepare the references from the valid captions \n",
        "        captions = captions_mapping[image_path]\n",
        "        references = prepare_references(captions)\n",
        "        references_list.append(references)\n",
        "\n",
        "        # The model generates a caption from a given image\n",
        "        image = decode_and_resize(image_path)\n",
        "        prediction, _ = generate_caption(image)\n",
        "        prediction = prediction.split()\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu1 = corpus_bleu(references_list, predictions, weights=(1.0, 0, 0, 0)) * 100\n",
        "    bleu2 = corpus_bleu(references_list, predictions, weights=(0.5, 0.5, 0, 0)) * 100\n",
        "    bleu3 = corpus_bleu(references_list, predictions, weights=(0.3, 0.3, 0.3, 0)) * 100\n",
        "    bleu4 = corpus_bleu(references_list, predictions, weights=(0.25, 0.25, 0.25, 0.25)) * 100\n",
        "    \n",
        "    print('BLEU-1: %f' % bleu1)\n",
        "    print('BLEU-2: %f' % bleu2)\n",
        "    print('BLEU-3: %f' % bleu3) \n",
        "    print('BLEU-4: %f' % bleu4)\n",
        "\n",
        "evaluate_model(valid_images, captions_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5fB_89htuBQ"
      },
      "outputs": [],
      "source": [
        "def plot_attention(image_path, result, attention_plot):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = np.array(image)\n",
        "  \n",
        "    result = result.split()\n",
        "    len_result = len(result)\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "\n",
        "    for i in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[i], (10, 10))\n",
        "        grid_size = max(int(np.ceil(len_result/2)), 2)\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
        "        ax.set_title(result[i])\n",
        "        img = ax.imshow(image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def show_img(img_path):\n",
        "  img = tf.io.read_file(img_path)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  img = tf.image.resize(img, IMAGE_SIZE)\n",
        "  img = img.numpy().clip(0, 255).astype(np.uint8)\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCzzZ5sm_Hjk"
      },
      "source": [
        "Run this cell multiple times to see results from the different images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulQ-wFLPwt5v"
      },
      "outputs": [],
      "source": [
        "valid_images = list(valid_data.keys())\n",
        "# Select a random image from the validation dataset\n",
        "image_path = np.random.choice(valid_images)\n",
        "\n",
        "# Read the image from the disk\n",
        "sample_img = decode_and_resize(image_path)\n",
        "result, attention_plot = generate_caption(sample_img)\n",
        "\n",
        "plot_attention(image_path, result, attention_plot)\n",
        "\n",
        "show_img(image_path)\n",
        "print('Predicted caption: ' + result)\n",
        "print('True captions:')\n",
        "for true_caption in captions_mapping[image_path]:\n",
        "  print(true_caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25LmQs8K5jh"
      },
      "source": [
        "### Data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhR2t0f0Knvd"
      },
      "outputs": [],
      "source": [
        "def calculate_bleu1(image_path, captions_mapping):\n",
        "    \"\"\"Calculate BLEU-1 for a description of an image \n",
        "    Args:\n",
        "        image_path (string): Path to the image\n",
        "        captions_mapping (dict): dictionary maps an image path to a list of captions\n",
        "    Return:\n",
        "        bleu1 (float): BLEU-1 score\n",
        "    \"\"\"\n",
        "    # Prepare the references from the valid captions \n",
        "    captions = captions_mapping[image_path]\n",
        "    references = prepare_references(captions)\n",
        "\n",
        "    # The model generates a caption from a given image\n",
        "    image = decode_and_resize(image_path)\n",
        "    prediction, _ = generate_caption(image)\n",
        "    hypothesis = prediction.split()\n",
        "\n",
        "    # Calculate BLEU-1 score\n",
        "    bleu1 = sentence_bleu(references, hypothesis, weights=(1.0, 0, 0, 0)) * 100\n",
        "\n",
        "    return bleu1, prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq14n2cOm9z9"
      },
      "source": [
        "Running this cell below could take about 10 minutes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJghINJmK9V_"
      },
      "outputs": [],
      "source": [
        "# Quality of captions\n",
        "exellent = {}\n",
        "high = {}\n",
        "medium = {}\n",
        "low = {}\n",
        "very_low = {}\n",
        "\n",
        "# List contains all the BLEU-1 Scores\n",
        "overall = []\n",
        "\n",
        "for img_path in valid_images:\n",
        "  bleu1, prediction = calculate_bleu1(img_path, captions_mapping)\n",
        "  overall.append(bleu1)\n",
        "\n",
        "  # Predicted captions divided into diff. groups depending on the BLEU-1 Score\n",
        "  if bleu1 > 80:\n",
        "    exellent[img_path] = [bleu1, prediction]\n",
        "  elif bleu1 > 60 and bleu1 <= 80:\n",
        "    high[img_path] = [bleu1, prediction]\n",
        "  elif bleu1 > 40 and bleu1 <= 60:\n",
        "    medium[img_path] = [bleu1, prediction]\n",
        "  elif bleu1 > 20 and bleu1 <= 40:\n",
        "    low[img_path] = [bleu1, prediction]\n",
        "  elif bleu1 <= 20:\n",
        "    very_low[img_path] = [bleu1, prediction]\n",
        "\n",
        "print(\"Number of exellent captions: \", len(exellent))\n",
        "print(\"Number of high quality captions: \",len(high))\n",
        "print(\"Number of medium quality captions: \",len(medium))\n",
        "print(\"Number of low quality captions: \",len(low))\n",
        "print(\"Number of very low captions: \",len(very_low))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKbqcYjQr5Kv"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "plt.hist(overall, 150)\n",
        "plt.xlabel('BLEU-1 Scores')\n",
        "plt.ylabel('Number of descriptions')\n",
        "plt.title('Data distribution of the BLEU-1 score in the valid set.')\n",
        "plt.legend()\n",
        "plt.savefig('bleu1_distribution.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWfP8GPWLZMp"
      },
      "outputs": [],
      "source": [
        "# Select the quality of the descriptions \n",
        "#and the number of descriptions to display.\n",
        "QUALITY = exellent\n",
        "N = 5\n",
        "\n",
        "for i, image_path in enumerate(QUALITY):\n",
        "  if i > N:\n",
        "    break\n",
        "  bleu1, caption = QUALITY[image_path]\n",
        "  show_img(image_path)\n",
        "  print(\"Path: \", image_path)\n",
        "  print(\"Caption: \", caption)\n",
        "  print(\"BLEU-1: \", '{0:.3g}'.format(bleu1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNBIsvzA3jyx"
      },
      "outputs": [],
      "source": [
        "# Select a random image from the internet\n",
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "\n",
        "# Decode the image & generate a caption\n",
        "sample_img = decode_and_resize(image_path)\n",
        "result, attention_plot = generate_caption(sample_img)\n",
        "\n",
        "plot_attention(image_path, result, attention_plot)\n",
        "\n",
        "show_img(image_path)\n",
        "print('Predicted caption: ' + result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVVasm0_3OBv"
      },
      "source": [
        "# References\n",
        "The implementation is partly based on these tutorials:\n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/image_captioning#create_a_tfdata_dataset_for_training\n",
        "\n",
        "https://keras.io/examples/vision/image_captioning/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Img_captioning_with_visual_attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
